#!/usr/bin/env python3
"""
Whisper è¯­éŸ³è½¬å½•è„šæœ¬
ä½¿ç”¨ OpenAI Whisper å°†è§†é¢‘/éŸ³é¢‘è½¬å½•ä¸º SRT å­—å¹•
"""

import sys
import os
import subprocess
import tempfile
from pathlib import Path

def check_dependencies():
    """æ£€æŸ¥å¿…è¦ä¾èµ–"""
    try:
        import whisper
        return True
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾èµ–: openai-whisper")
        print("å®‰è£…å‘½ä»¤: pip install openai-whisper")
        return False

def extract_audio(video_path: str, audio_path: str) -> bool:
    """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘"""
    try:
        cmd = [
            "ffmpeg", "-y", "-i", video_path,
            "-vn", "-acodec", "pcm_s16le",
            "-ar", "16000", "-ac", "1",
            audio_path
        ]
        subprocess.run(cmd, check=True, capture_output=True)
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ éŸ³é¢‘æå–å¤±è´¥: {e}")
        return False

def format_timestamp(seconds: float) -> str:
    """æ ¼å¼åŒ–æ—¶é—´æˆ³ä¸º SRT æ ¼å¼"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

def transcribe(video_path: str, output_srt: str, model_name: str = "medium",
               language: str = "auto", device: str = "cuda"):
    """
    æ‰§è¡Œè½¬å½•
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        output_srt: è¾“å‡º SRT æ–‡ä»¶è·¯å¾„
        model_name: Whisper æ¨¡å‹ (tiny/base/small/medium/large)
        language: è¯­è¨€ (zh/en/auto)
        device: è®¾å¤‡ (cuda/cpu)
    """
    import whisper
    import torch
    
    # æ£€æŸ¥ CUDA å¯ç”¨æ€§
    if device == "cuda" and not torch.cuda.is_available():
        print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œå›é€€åˆ° CPU")
        device = "cpu"
    
    print(f"ğŸ“¥ åŠ è½½ Whisper {model_name} æ¨¡å‹...")
    model = whisper.load_model(model_name, device=device)
    
    # æå–éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
        audio_path = tmp.name
    
    print("ğŸµ æå–éŸ³é¢‘...")
    if not extract_audio(video_path, audio_path):
        return False
    
    print("ğŸ¤ è½¬å½•ä¸­...")
    options = {"task": "transcribe"}
    if language != "auto":
        options["language"] = language
    
    result = model.transcribe(audio_path, **options)
    
    # ç”Ÿæˆ SRT æ–‡ä»¶
    print("ğŸ“ ç”Ÿæˆå­—å¹•æ–‡ä»¶...")
    with open(output_srt, "w", encoding="utf-8") as f:
        for i, segment in enumerate(result["segments"], 1):
            start = format_timestamp(segment["start"])
            end = format_timestamp(segment["end"])
            text = segment["text"].strip()
            f.write(f"{i}\n{start} --> {end}\n{text}\n\n")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    os.unlink(audio_path)
    
    print(f"âœ… è½¬å½•å®Œæˆ: {output_srt}")
    print(f"   æ£€æµ‹è¯­è¨€: {result.get('language', 'unknown')}")
    print(f"   ç‰‡æ®µæ•°é‡: {len(result['segments'])}")
    
    return True

def main():
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python transcribe_audio.py <è§†é¢‘è·¯å¾„> <è¾“å‡ºSRTè·¯å¾„> [æ¨¡å‹] [è¯­è¨€] [è®¾å¤‡]")
        print("æ¨¡å‹: tiny/base/small/medium/large (é»˜è®¤: medium)")
        print("è¯­è¨€: zh/en/auto (é»˜è®¤: auto)")
        print("è®¾å¤‡: cuda/cpu (é»˜è®¤: cuda)")
        sys.exit(1)
    
    if not check_dependencies():
        sys.exit(1)
    
    video_path = sys.argv[1]
    output_srt = sys.argv[2]
    model_name = sys.argv[3] if len(sys.argv) > 3 else "medium"
    language = sys.argv[4] if len(sys.argv) > 4 else "auto"
    device = sys.argv[5] if len(sys.argv) > 5 else "cuda"
    
    if not os.path.exists(video_path):
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        sys.exit(1)
    
    success = transcribe(video_path, output_srt, model_name, language, device)
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
